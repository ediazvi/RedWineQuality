---
title: 'Tipologia y ciclo de vida de los datos - PRA2'
author: "Autor: Eduardo Diaz Villanueva e Ignasi Domingo González"
date: "Enero 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Descripción del dataset.

```{r }
# Cargamos las diferentes librerias que utilizaremos.
packages = c('stringr', 
             'dplyr', 
             'ggplot2', 
             'corrplot', 
             'car', 
             'vcd', 
             'rpart', 
             'rpart.plot', 
             'tidyverse', 
             'caret' )

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```

```{r }
# Limpiamos la aplicación de datos anteriores y cargo el fichero.
rm(list = ls())
datos <- read.csv("winequality-red.csv", sep=",")
datos_originales <- datos
# Vemos el tamaño del conjunto de datos
dim(datos)
# Visualizamos los primeros elementos del conjunto
head(datos,5)
```

El dataset seleccionado contiene 11 variables que describen las propiedades químicas de un vino, como puede ser la acidez, ph nivel de azúcar, etc... estas variables tendrán influencia en la calidad final del vino. Además contiene un atributo que es la calidad del vino, como ha sido clasificado.

Con este ejercicio queremos estudiar que variables son mas representativas y encontrar modelos que puedan predecir la calidad del vino.

Si pensamos por ejemplo en una industria, podríamos reducir el tiempo y coste reduciendo el numero de pruebas de calidad a las variables mas significativas. Incluso mejorar la calidad del producto final, focalizando esfuerzos y recursos a reducir la variabilidad de las variables que mas contribuyan a la calidad final.

# Integracion y seleccion de los datos de interes

Realizaremos un primer análisis estadístico para familiarizarnos con las variables y sus tipos de datos.
````{r }
#Resumen del conjunto de datos.
summary(datos)
# Visualización de los primeros valores de cada atributo.
str(datos)
#Tipo de dato asignado a cada campo
sapply(datos, function(x) class(x))
```
Observamos que los tipos de datos asignados a las variables corresponden con las variables que representan.
# Limpieza de los datos
## Elementos vacios
Analizamos los valores de las variables para detectar falta o ausencia de datos
````{r }
# Analizamos la existencia de datos NA
colSums(is.na(datos))
```
````{r }
# Analizamos la existencia de datos vacios
colSums(datos=="")
```
````{r }
# Analizamos la existencia de datos con valor 0
colSums(datos==0)
```
Observamos la variable "Citric.acid" con una gran cantidad de valores 0.
Tras realizar una pequeña investigación, (https://es.wikipedia.org/wiki/%C3%81cidos_en_el_vino#%C3%81cido_c%C3%ADtrico) podemos deducir que en la uva, el ácido cítrico es un componente presente de forma natural pero con muy baja presencia, lo que si no se añade posteriormente puede presentar valores de 0.
Consideramos que dicha variable tiene valores correctos y no requiere de ninguna acción.

## Elementos duplicados
Dado que no hay presentes registros con elementos vacios, vamos a verificar si hay registros duplicados.
````{r }
# Analizamos la existencia de registros duplicados.
sum(duplicated(datos))
```
Algunas filas están duplicadas. A modo de ejemplo, la fila 1 y la fila 5 son la misma.
````{r }
# Ejemplo de registro duplicado
a <- datos %>% filter(row_number() == 1) 
b <- datos %>% filter(row_number() == 5)
a == b
```
Los registros duplicados no nos dan más información sobre la muestra, por lo que vamos a eliminar las filas duplicadas.
````{r }
# Eliminamos los registros duplicados
datos <- datos[!duplicated(datos), ]
```
````{r }
# Revisamos el tamaño de nuestro nuevo dataset
dim(datos)
```
## Valores extremos 
Analizaremos individualmente cada una de las variables focalizándonos en la distribución de los datos y sus valores extremos.
````{r }
par(mfrow=c(1,2))
hist(datos$fixed.acidity, breaks  = 30)
boxplot(datos$fixed.acidity,main="fixed.acidity", col="lightblue")
boxplot.stats(datos$fixed.acidity)$out
```
Observamos como el atributo "fixed.acidity" tiene 41 valores extremos, distribuidos entre 12.4 y 16
````{r }
par(mfrow=c(1,2))
hist(datos$volatile.acidity, breaks  = 30)
boxplot(datos$volatile.acidity,main="volatile.acidity", col="lightblue")
boxplot.stats(datos$volatile.acidity)$out
```
Observamos como el atributo "volatile.acidity" tiene 19 valores extremos, distribuidos entre 1 y 1.6
````{r }
par(mfrow=c(1,2))
hist(datos$citric.acid , breaks  = 30)
boxplot(datos$citric.acid ,main="citric.acid ", col="lightblue")
boxplot.stats(datos$citric.acid )$out
```
Observamos como el atributo "citric.acid" tiene un valor extremo de valor 1.
````{r }
par(mfrow=c(1,2))
hist(datos$residual.sugar, breaks  = 30)
boxplot(datos$residual.sugar,main="residual.sugar", col="lightblue")
boxplot.stats(datos$residual.sugar)$out
```
Observamos como el atributo "residual.sugar" tiene 126 valores extremos, distribuidos entre 4 y 16
````{r }
par(mfrow=c(1,2))
hist(datos$chlorides, breaks  = 50)
boxplot(datos$chlorides,main="chlorides", col="lightblue")
boxplot.stats(datos$chlorides)$out
```
Observamos como el atributo "chlorides" tiene 87 valores extremos, distribuidos entre 0 y 0.05 por la parte inferior y entre 0.12 y 0.6 por la parte superior.
````{r }
par(mfrow=c(1,2))
hist(datos$free.sulfur.dioxide, breaks  = 30)
boxplot(datos$free.sulfur.dioxide,main="free.sulfur.dioxide", col="lightblue")
boxplot.stats(datos$free.sulfur.dioxide)$out
```
Observamos como el atributo "free.sulfur.dioxide" tiene 26 valores extremos, distribuidos entre el 43 y el 60
````{r }
par(mfrow=c(1,2))
hist(datos$total.sulfur.dioxide, breaks  = 30)
boxplot(datos$total.sulfur.dioxide,main="total.sulfur.dioxide", col="lightblue")
boxplot.stats(datos$total.sulfur.dioxide)$out
```
Observamos como el atributo "total.sulfur.dioxide" tiene 45 valores extremos, distribuidos entre el 120 y el 300.
````{r }
par(mfrow=c(1,2))
hist(datos$density, breaks  = 30)
boxplot(datos$density,main="density", col="lightblue")
boxplot.stats(datos$density)$out
```
Observamos como el atributo "density" tiene 35 valores extremos, distribuidos entre 0.990 y 0.992 por la parte inferior y entre 1.001 y 1.004 por la parte superior.
````{r }
par(mfrow=c(1,2))
hist(datos$pH, breaks  = 30)
boxplot(datos$pH,main="pH", col="lightblue")
boxplot.stats(datos$pH)$out
```
Observamos como el atributo "pH" tiene 28 valores extremos, distribuidos entre 2.7 y 2.9 por la parte inferior y entre 3.7 y 4 por la parte superior.
````{r }
par(mfrow=c(1,2))
hist(datos$sulphates, breaks  = 30)
boxplot(datos$sulphates,main="sulphates", col="lightblue")
boxplot.stats(datos$sulphates)$out
```
Observamos como el atributo "sulphates" tiene 55 valores extremos, distribuidos entre 1 y 2.
````{r }
par(mfrow=c(1,2))
hist(datos$alcohol, breaks  = 30)
boxplot(datos$alcohol,main="alcohol", col="lightblue")
boxplot.stats(datos$alcohol)$out
```
Observamos como el atributo "alcohol" tiene 12 valores extremos, distribuidos entre el 13.5 y el 14.
````{r }
par(mfrow=c(1,2))
hist(datos$quality, breaks  = 30)
boxplot(datos$quality,main="quality", col="lightblue")
boxplot.stats(datos$quality)$out
```
Observamos como el atributo "quality" tiene 27 valores extremos, distribuidos entre el 3 por la parte inferior, y el 8 en la parte superior.
Este valor es una valoración del vino, por lo que estos valores no se pueden considerar extremos.

En conclusión, con el análisis realizado para cada variable, el número de valores extremos es muy dispar, siendo bajo en algunas variables y relativamente alto en otras.
Como es posible que algunos valores extremos de una variable coincidan en registro con los de otra, para identificar correctamente los valores extremos vamos a utilizar la distancia de Cook, estimando el grado de influencia de cada uno de los valores al realizar un análisis de regresión por mínimos cuadrados.
Para realizarlo, tendremos en cuenta todos los atributos a excepción de "quality".
````{r }
# Cálculo y visualización de resultados de aplicar la distancia de Cook a nuestros datos.
outliers = c()
for ( i in 1:11 ) {
  stats = boxplot.stats(datos[[i]])$stats
  bottom_outlier_rows = which(datos[[i]] < stats[1])
  top_outlier_rows = which(datos[[i]] > stats[5])
  outliers = c(outliers , top_outlier_rows[ !top_outlier_rows %in% outliers ] )
  outliers = c(outliers , bottom_outlier_rows[ !bottom_outlier_rows %in% outliers ] )
}
mod = lm(quality ~ ., data = datos)
cooksd = cooks.distance(mod)
plot(cooksd, pch = "*", cex = 2, 
     main = "Observaciones relevantes en función de la distancia de Cook")
abline(h = 4*mean(cooksd, na.rm = T), col = "red")
```
````{r }
# Obtenemos el listado de los valores extremos que afectan sensiblemente a los datos.
head(datos[cooksd > 4 * mean(cooksd, na.rm=T), ])
```
Si visualizamos las primeras entradas, todos ellos tienen valores atípicos en una o más variables. 
El registro 14 tiene los "chlorides" y los "sulphates" muy altos.
El registro 34 tiene el "residual.sugar" muy alto.
El registro 46 tiene el "pH" muy alto.
El registro 80 tiene los "sulphates" altos.
El registro 87 y 93 tienen los "chlorides", los "sulphates" y el "total.sulfur.dioxide" altos.


Vamos a eliminar los valores extremos detectados.
````{r }
# Identificamos los registros a eliminar
coutliers = as.numeric(rownames(datos[cooksd > 4 * mean(cooksd, na.rm=T), ]))
outliers = c(outliers , coutliers[ !coutliers %in% outliers ] )

# Eliminamos los elementos detectados como extremos.
datos_clean = datos[-outliers, ]

# Visualizamos el tamaño y los valores básicos de nuestro nuevo conjunto de datos.
dim(datos_clean)
summary(datos_clean)
```
# Análisis de los datos
Antes de comenzar con el análisis guardaremos una copia de los datos después del proceso de limpieza
````{r }
# Exportación de los datos limpios en .csv
write.csv(datos_clean, "winequality-red-clean.csv")
```
Analizaremos las variables frente a la calidad para decidir cuales utilizar en el resto del análisis
````{r }
boxplot(formula = datos_clean$fixed.acidity ~ datos_clean$quality, 
        main="fixed.acidity vs quality", col="lightblue")
boxplot(formula = datos_clean$volatile.acidity ~ datos_clean$quality, 
        main="volatile.acidity vs quality", col="lightblue")
boxplot(formula = datos_clean$citric.acid ~ datos_clean$quality, 
        main="citric.acid vs quality", col="lightblue")
boxplot(formula = datos_clean$residual.sugar ~ datos_clean$quality, 
        main="residual.sugar vs quality", col="lightblue")
boxplot(formula = datos_clean$chlorides ~ datos_clean$quality, 
        main="chlorides vs quality", col="lightblue")
boxplot(formula = datos_clean$free.sulfur.dioxide ~ datos_clean$quality, 
        main="free.sulfur.dioxide vs quality", col="lightblue")
boxplot(formula = datos_clean$total.sulfur.dioxide ~ datos_clean$quality, 
        main="total.sulfur.dioxide vs quality", col="lightblue")
boxplot(formula = datos_clean$density ~ datos_clean$quality, 
        main="density vs quality", col="lightblue")
boxplot(formula = datos_clean$pH ~ datos_clean$quality, 
        main="pH vs quality", col="lightblue")
boxplot(formula = datos_clean$sulphates ~ datos_clean$quality, 
        main="sulphates vs quality", col="lightblue")
boxplot(formula = datos_clean$alcohol ~ datos_clean$quality, 
        main="alcohol vs quality", col="lightblue")
```
## Seleccion grupo de datos
De la observación del grupo de datos nos interesa seleccionar los que pudieran tener una mayor relación con el resultado de calidad. Por ello vamos a seleccionar las que se intuye una cierta relación lineal para poder aplicar modelos de predicción.
Las variables "citrix acid" , "alcohol" y "sulphates", conforme aumentan, aumenta el valor de la calidad. 
Por el contrario para que aumente el valor de la calidad es necesario que disminuyan "volatile acidity", "density".
Crearemos un subconjunto de datos con estas cinco variables
````{r }

subdatos <- select(datos_clean, 
                   volatile.acidity, 
                   citric.acid, 
                   sulphates, 
                   alcohol, 
                   density, 
                   quality)
```


## Comprobación de la normalidad y homogeneidad de la varianza.
Existen diferentes maneras de comprobar la normalidad de los datos, la del test de Shapiro es la más habitual, pero también es posible realizar dicha comprobación de forma visual mediante los histogramas y las gráficas quantilie-quantile. Este método nos permite identificar más fácilmente las distribuciones que se alejan de la normalidad.
````{r }
# Gráficas QQ de comprobación de normalidad e histogramas
par(mfrow=c(3,2))
for (i in 1:(ncol(subdatos)-1)) {
  hist(subdatos[[i]], xlab = names(subdatos)[i], col = 'lightblue', 
       main = paste("Average =", signif(mean(subdatos[[i]]),3)), breaks = 50)
  qqnorm(subdatos[,i], main = colnames(subdatos[i]))
  qqline(subdatos[,i])
  
}
```

Los resultados gráficos nos indican que las variables "volatile.acidity", "sulphates" y "density" podrían tener una distribución normal.
Las variables "citric.acid" y "alcohol" no presentan visualmente una distribución normal.la mayoría de los atributos se acercan mucho a una distribución normal.

Vamos a verificar la normalidad de los datos con un test de normalidad para cada valor. El test asume como hipótesis nula la distribución normal de los datos.

````{r }
# Test de normalidad para las diferentes variables de nuestro subconjunto de datos
shapiro.test(subdatos$volatile.acidity)
shapiro.test(subdatos$citric.acid)
shapiro.test(subdatos$sulphates)
shapiro.test(subdatos$alcohol)
shapiro.test(subdatos$density)
```
Observamos que p-value está por debajo del valor de significancia, para todas las variables a excepción de "density".
La única varible que se puede considerar que tiene una distribución normal de los datos es "density".


Para la verificación de la homocedasticidad, vamos a utilizar el test de Fligner-Killen, que se puede aplicar sobre datos que no cumplen con la condición de normalidad.
El test asume como hipótesis nula la igualdad de varianzas en los diferentes grupos de datos.

````{r }
# Visualización de las distribuciones de datos de densidad en función de la calidad.
fligner.test(volatile.acidity ~ quality, data = subdatos)
fligner.test(citric.acid ~ quality, data = subdatos)
fligner.test(sulphates ~ quality, data = subdatos)
fligner.test(alcohol ~ quality, data = subdatos)
fligner.test(density ~ quality, data = subdatos)
```
En este caso, observamos que las variables "volitile.acidity", y "density" tiene un p-value por encima del nivel de significancia, lo que nos indica que si presentan homocedasticidad, en el resto de variables el valor está por debajo del nivel de significancia, por lo que dichas variables presentan varianzas estadísticamente diferentes para los diferentes grupos de "quality".

En el caso de que se cumplan ambas premisas, podemos realizar una comparación entre los dos grupos de datos mediante una prueba t de Student.
La test asume como hipótesis nula que las medias de los grupos de datos son las mismas.

Como tenemos que comparar datos de dos conjuntos, vamos a crear un subconjunto de datos con dos categorías de vinos, y los compararemos entre ellos.


````{r }
# Visualización de las distribuciones de datos de densidad en función de la calidad.
ggplot(data = subdatos, aes(x = quality, y = density, colour = quality, group=quality)) +
  geom_boxplot() +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none")

# Selección de los conjuntos a comparar según la calidad del vino.
calidad <- filter(.data = subdatos, quality %in% c("4", "7"))

# Test que nos permite comprobar si se observan diferencias estadísticamente 
# significativas de la densidad entre vinos de diferentes calidades escogidos.
t.test(density ~ quality, data = calidad)


```
En este caso tenemos un valor de p-value menor al nivel de significancia, lo que significa que se observan diferencias estadísticamente significativas entre los dos grupos de datos de la calidad del vino escogidos para la variable densidad. 


Para el resto de variables, podemos aplicar las pruebas de Wilcox, ya que no requieren de la premisa de normalidad y homocedasticidad.
Vamos a ver la como lo aplicamos para la variable "alcohol".

````{r }
# Visualización de las distribuciones de datos de densidad en función de la calidad.

ggplot(data = subdatos, aes(x = quality, y = alcohol, colour = quality, group=quality)) +
  geom_boxplot() +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none")

# Selección de los conjuntos a comparar según la calidad del vino.
calidad <- filter(.data = subdatos, quality %in% c("4", "7"))

# Test que nos permite comprobar si se observan diferencias estadísticamente 
# significativas de la densidad entre vinos de diferentes calidades escogidos.
wilcox.test(alcohol ~ quality, data = calidad)


```

En este caso, podemos observar que p-value está por debajo del nivel de significancia, por lo que sí se observan diferencias estadísticamente significativas en la calidad del vino en terminos del alcohol presente.



Nuestro objetivo es la predicción de la calidad del vino con los diferentes parámetros de sus componentes, por lo que que vamos a proseguir con el análisis de correlación de los datos.


## Aplicación de pruebas estadísticas para comparar los grupos de datos
Aplicaremos diversas pruebas estadísticas para analizar la relación de los datos y poder crear el modelo de predicción de la calidad.
Comenzaremos por analizar los valores de correlación de las variables con la variable "quality"
### Correlacion

Vamos a analizar la correlación entre las variables.
````{r }
# Visualizaremos la matriz de correlación de variables de todo el conjunto de datos.
correlacion_dc<-round(cor(datos_clean), 1)
corrplot(correlacion_dc, method="number", type="upper")
```
````{r }
# Visualizaremos la matriz de correlación de las variables del subconjunto seleccionado.
correlacion_sc<-round(cor(subdatos), 1)
corrplot(correlacion_sc, method="number", type="upper")
```
Guardaremos los datos de correlación en una matriz ordenada para decir que variables utilizar en siguientes estudios.
Dado que hemos observado que no siempre se cumple el criterio de homocedasticidad, vamos a utilizar la correlación de Spearman, ya que este método no conlleva ninguna suposición sobre la distribución de datos.
````{r }
# Creamos la matriz para almacenar los datos
matrixcor <- matrix(nc = 2, nr = 0)
colnames(matrixcor) <- c("Variable","correlacion")
# Recorremos el dataset ejecutando el test
for (i in 1:(ncol(subdatos)-1)) {
  
  coef <- cor(x=subdatos$quality, y = subdatos[,i], method="spearman")
  # Añadimos los datos a la matriz
  pair = matrix(ncol = 2, nrow = 1)
  pair[1][1] = colnames(subdatos[i])
  pair[2][1] = coef
  matrixcor <- rbind(matrixcor, pair)
}
# Ordenamos por el valor de correlacion
matrixcor[order(matrixcor[,"correlacion"]), ]



```
Observamos como algunos valores tiene una correlación positiva importante con la calidad del vino. Igualmente podemos observar como existen valores con una relevante correlación negativa, que también deben ser considerados.

### Regresión lineal

Con este grupo de datos y las relaciones observadas tanto en las gráficas de caja como los datos de correlación estimaremos por mínimos cuadrados ordinarios un modelo lineal que explique la variable "quality". Vamos a tener en cuenta todas las variables que tengan un valor de correlación superior a 0.2, tanto positivas como negativas.
````{r }
# Creamos el modelo 
modelo <- (lm(formula = quality ~ 
                alcohol +
                sulphates +
                citric.acid +
                volatile.acidity +
                density, 
              data = subdatos))
summary(modelo)
```
Podemos observar que las variables "citric.acid" y "density" son variables poco significativas para el modelo. Con un valor de R cuadraddo ajustado del 37.69%. Si lo comparamos con la tabla de correlación, solo se han considerado las variables con un valor de correlación superior a 0.3
````{r }
# Creamos un modelo reducido 
modelo_reducido <- (lm(formula = quality ~ alcohol + sulphates + volatile.acidity, 
                       data = subdatos))
summary(modelo_reducido)
```
Como podemos observar, todas las variables utilizadas se consideran significativas, y la exclusión de las tres variables no significativas respecto al modelo anterior no ha supuesto una merma relevante en la calidad del modelo, con estas tres variables podemos explicar el 37.56% de la clasificación de quality.
Finalmente, analizamos estadísticamente el modelo.
````{r }
par(mfrow=c(2,2))
plot(modelo_reducido)
```
La gráfica de residuos frente a Fitted muestra si los residuos tienen patrones no lineales. Los residuos alrededor de una línea horizontal sin patrones distintos indican que tenemos relaciones lineales.
La gráfica QQ plot normal muestra los residuos que se ajustan a la línea normal distribuidos.
La grafica Scale-Location muestra si los residuos se distribuyen por igual a lo largo de los rangos de predictores de forma que podemos verificar el supuesto de varianza igual (homocedasticidad). Podemos observar una linea  horizontal con puntos de dispersión iguales.
El gráfico de residuos vs apalancamiento tiene un aspecto típico cuando hay algún caso influyente. Apenas puede ver las líneas de distancia de Cook (una línea punteada roja) porque casi todos los casos están dentro de la distancia de Cook.


Vamos a generar el modelo de regresión lineal con los dos conjuntos de datos, uno de entrenamiento y otro de test, con el objetivo de ver como de bueno es el modelo.
````{r }

# Generamos los dos conjuntos de datos
set.seed(1701)
trainIndex <- createDataPartition(subdatos$quality, p = 0.8, list = FALSE)
train_data <- subdatos[trainIndex, ]
test_data <- subdatos[-trainIndex, ]


# Creamos un modelo reducido 
modelo_reducido_lm <- (lm(formula = quality ~ alcohol + sulphates + volatile.acidity, 
                          data = train_data))
summary(modelo_reducido_lm)

# Predicción sobre los datos de test
predicciones_lm <- predict(modelo_reducido_lm, test_data)
plot(test_data$quality, predicciones_lm)

# Creamos la matriz de confusión de los valores
matriz_confusion_lm <- table(test_data$quality , 
                             round(predicciones_lm), 
                             dnn = c("observaciones", "predicciones"))
matriz_confusion_lm

# Matriz de confusión del modelo de regresión lineal con los tres parámetros significativos.
mosaic(matriz_confusion_lm, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("red2",   "green3",   "red2",   "red2",   "red2",
                                 "red2",   "red2",     "green3", "red2",   "red2", 
                                 "red2",   "red2",     "red2",   "green3", "red2"), 5, 3)))
```
Podemos observar que los valores obtenidos en la predicción están dentro del rango de valores esperado, por lo que el modelo está realizando correctamente los cálculos.
En este caso observamos a partir de la matriz de confusión como el modelo tiene aproximadamente un 62% de acierto en la predicción.

La interpretación del modelo sería una equación lineal, que se puede recrear a partir de los parámetros obtenidos por el modelo.

prediccion = 1.771545 + (0.311126 * alcohol) + (1.714843 * sulphates) - (0.846597  * volatile.acidity)


### Regresión logistica
Con el objetivo de tener un modelo que nos permitiera tener un control de calidad excluyente y que nos permitiera decidir si el producto final está dentro de los vinos considerados de alta calidad, con valoración de 6, 7 u 8. Vamos a crear un modelo de regresión logística y compararemos resultados.
````{r }
# Creamos una variable calidad de tipo factor
subdatos$quality_factor <- as.factor(subdatos$quality)

# Creamos un variable calidad binomial donde especificamos que queremos vinos 
# con valoraciones de 6 o más.
subdatos$category[subdatos$quality < 6] <- 0
subdatos$category[subdatos$quality >= 6] <- 1
subdatos$category <- as.factor(subdatos$category)
```

Creamos el modelo de regresión logística
````{r }
# Generamos el modelo con las variables seleccionadas
modelo_log <- glm(category ~ alcohol + sulphates + volatile.acidity, 
                  data = subdatos, family = "binomial")
summary(modelo_log)
```


Creamos la tabla de confusión correspondiente al modelo.
```{r }
predicciones <- ifelse(test = modelo_log$fitted.values > 0.5, yes = 1, no = 0)
matriz_confusion <- table(modelo_log$model$category , 
                          predicciones, 
                          dnn = c("observaciones", "predicciones"))
matriz_confusion
```
Visualizamos los resultados.
```{r }
mosaic(matriz_confusion, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("green3", "red2", "red2", "green3"), 2, 2)))

```
El modelo de regresión logística nos ha dado un resultado de predicción de un 73% de acierto.
Hemos de tener en cuenta que hemos simplificado la predicción a dos valores, considerando los vinos con puntuación de 5 o menos como malos y de 6, 7 u 8 como buenos.
En este caso, hemos entrenado el modelo con todos los datos y lo hemos evaluado sobre los mimos datos. 


Vamos a crear un modelo predictivo sobre la variable quality, dividiendo los datos en un conjunto de entrenamiento y otro de test.
En este caso, vamos a intentar precedir la calidad del vino en todas sus categorias en función de los parámetros.

```{r }

# Generamos los dos conjuntos de datos
set.seed(1234)
trainIndex <- createDataPartition(subdatos$quality, p = 0.8, list = FALSE)
train_data <- subdatos[trainIndex, ]
test_data <- subdatos[-trainIndex, ]

# Generamos nuestro modelo de regresión logística con las variables escogidas
modelo_reducido_glm <- glm(quality ~ alcohol + sulphates + volatile.acidity, 
                           data = train_data, family = "gaussian")
summary(modelo_reducido_glm)

```

Probamos el modelo con los datos de test. Las predicciones son de valores continuos, y en nuestro caso tenemos valores categóricos, por lo que realizaremos un ajuste del resultado por redondeo para obtener la matriz de confusión.

```{r }
# Predicción sobre los datos de test
predicciones_glm <- predict(modelo_reducido_glm, test_data)
plot(test_data$quality, predicciones_glm)

# Creamos la matriz de confusión de los valores
matriz_confusion_glm <- table(test_data$quality , 
                              round(predicciones_glm), 
                              dnn = c("observaciones", "predicciones"))
matriz_confusion_glm

# Visualizamos la matriz de confusión
mosaic(matriz_confusion_glm, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("red2",   "green3",   "red2",   "red2",   "red2",  
                                 "red2",   "red2",     "green3", "red2",   "red2", 
                                 "red2",   "red2",     "red2",   "green3", "red2"), 5, 3)))


```
Podemos observar que los valores obtenidos en la predicción están dentro del rango de valores esperado, por lo que el modelo está realizando correctamente los cálculos.
En este caso observamos a partir de la matriz de confusión como el modelo tiene un 61,34% de acierto en la predicción.


### Modelos de clasificacion
También nos seria útil tener algún modelo de clasificación que pudiéramos determinar la calidad del vino en función de sus características. Podría agrupar los productos o producción en varios productos de venta, etc...
Crearemos un modelo supervisado.


#### Arbol de decision
Necesitaremos prepara los datos para crear un grupo de datos de entrenamiento y de prueba.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Copiamos los datos y eliminamos algunas columnas
datos_arbol <- subdatos
datos_arbol$quality <- NULL 
datos_arbol$quality_factor <- NULL 
#datos_arbol$category <- NULL 
# Creamos los conjuntos de datos de entrenamiento y de test.
set.seed(666)
datos_training <- sample_frac(datos_arbol, .7)
datos_test <- setdiff(datos_arbol, datos_training)
```
Entrenamos el modelo basándonos en la variable categoria
```{r echo=TRUE, message=FALSE, warning=FALSE}
arbol <- rpart(formula = category~ ., data = datos_training)
```
Evaluando el modelo
```{r echo=TRUE, message=FALSE, warning=FALSE}
arbol
```
Mostramos el arbol de desicion con la funcion plot
```{r echo=TRUE, message=FALSE, warning=FALSE}
rpart.plot(arbol)
```
Creamos la matrix de confusion para evaluar el arbol.
```{r echo=TRUE, message=FALSE, warning=FALSE}
prediccion <- predict(arbol, newdata = datos_test, type = "class")
confusionMatrix(prediccion, datos_test[["category"]])
confusionMatrix_table <- confusionMatrix(prediccion, datos_test[["category"]])$table

mosaic(confusionMatrix_table, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("green3", "red2", "red2", "green3"), 2, 2)))
```
Vemos que la precisión del modelo es 69.73%. Con un valor del factor Kappa de 0.3946



# Representación de los resultados

A partir del conjunto de datos inicial, y tras una revisión y eliminación de valores perdidos, elementos duplicados y valores extremos, se ha obtenido un conjunto de datos limpios con el cual empezar a analizar.
````{r }
summary(datos_clean)
```
Hemos analizado la distribución de los datos en función de la variable "quality" y hemos identificado visualmente los elementos que afectan significativamente a la calidad del vino, tanto positivamente como negativamente.

````{r }
par(mfrow=c(2,3))

boxplot(formula = datos_clean$sulphates ~ datos_clean$quality, 
        main="sulphates vs quality", col="lightblue")
boxplot(formula = datos_clean$alcohol ~ datos_clean$quality, 
        main="alcohol vs quality", col="lightblue")
boxplot(formula = datos_clean$citric.acid ~ datos_clean$quality, 
        main="citric.acid vs quality", col="lightblue")
boxplot(formula = datos_clean$volatile.acidity ~ datos_clean$quality, 
        main="volatile.acidity vs quality", col="lightblue")
boxplot(formula = datos_clean$density ~ datos_clean$quality, 
        main="density vs quality", col="lightblue")

```
Hemos analizado la normalidad y homocedasticidad de los datos de este subconjunto.

````{r }
# Gráficas QQ de comprobación de normalidad e histogramas
par(mfrow=c(3,2))
for (i in 1:5) {
  hist(subdatos[[i]], 
       xlab = names(subdatos)[i], 
       col = 'lightblue', 
       main = paste("Average =", signif(mean(subdatos[[i]]),3)), 
       breaks = 50)
  qqnorm(subdatos[,i], main = colnames(subdatos[i]))
  qqline(subdatos[,i])
  
}

```



Hemos analizado la correlación entre variables.
````{r }
corrplot(correlacion_sc, method="number", type="upper")
```
Con toda la información analizada, se ha decidido el analizar la creación de modelos de predicción y clasificación con estas variables. 

Tras una primera versión de un modelo de regresión lineal, se ha detectado que el conjunto de variables siginificativas se puede reducir a tres, "alcohol", "sulphates" y "Volatile.acidity", sin que afecte al resultado del modelo.

Con estas variables hemos realizado varios modelos.

````{r }
# Matriz de confusión del modelo de regresión lineal con los tres parámetros significativos.
mosaic(matriz_confusion_lm, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("red2",   "green3",   "red2",   "red2",   "red2",  
                                 "red2",   "red2",     "green3", "red2",   "red2", 
                                 "red2",   "red2",     "red2",   "green3", "red2"), 5, 3)))
```
En este caso observamos a partir de la matriz de confusión como el modelo tiene aproximadamente un 62% de acierto en la predicción.


````{r }
# Matriz de confusión del modelo de regresión logística con los tres parámetros significativos.
mosaic(matriz_confusion_glm, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("red2",   "green3",   "red2",   "red2",   "red2",  
                                 "red2",   "red2",     "green3", "red2",   "red2", 
                                 "red2",   "red2",     "red2",   "green3", "red2"), 5, 3)))

```
En este caso observamos a partir de la matriz de confusión como el modelo tiene un 61,34% de acierto en la predicción.

Ambos modelos de regresión nos das resultados muy similares.

Vamos a probar con un modelo de clasificación por árbol de decisión.

````{r }
# Matriz de confusión del modelo de clasificación por árbol de decisión con los tres 
# parámetros significativos
mosaic(confusionMatrix_table, 
       shade = T, 
       colorize = T, 
       gp = gpar(fill = matrix(c("green3", "red2", "red2", "green3"), 2, 2)))
```
Vemos que en este caso hemos mejorado la precisión del modelo al aumentarla hasta el 69.73%.



# Resolución del problema

Como conclusión, podemos decir que hay tres variables más representativas que el resto en la clasificación de calidad del vino, estas son "alcohol", "sulphates" y "Volatile.acidity".

A partir de dichas variables, y mediante un modelo de clasificación por árbol de decisión, podemos obtener con una predicción de la calidad del vino, con un nivel de acierto del 70%.









































